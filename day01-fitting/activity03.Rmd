---
title: "Activity 2 - Day 1"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Task2 : Loading the necessary packages

```{r Loading_packages}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidymodels))
suppressPackageStartupMessages(library(GGally))
suppressPackageStartupMessages(library(scatterplot3d))

```

## Task3 : Loading the data

```{r Loading_dataset}
hfi<-readr::read_csv("https://www.openintro.org/data/csv/hfi.csv",show_col_types = FALSE)
```

1) This is an observational study as it was not using or conducting any experiment to produce the reports. 

## Plot to visualize the distribution of pf_score

```{r}
plot1 <- hfi %>% ggplot(aes(x=pf_score))+
  geom_histogram(fill="dodgerblue",color="black")+
  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10))+
  scale_y_continuous(breaks = c(0,20,40,60,80,100))+
  theme_bw()

plot1
```

2) The univariate histogram for pf_score is left_skewed with extreme outliers on the left (very low pf_scores) and multiple peaks. I expected that a lot of countries will be having low pf_scores but surprisingly it was the opposite.


3) I have decided to describe the relation between pf_rol_civil and pf_rol_criminal which looks Linear relationship for me and displayed below with the help of a scatter plot and linear regression line.

```{r}
plot2<- hfi %>% ggplot(aes(x=pf_rol_criminal,y=pf_rol_civil))+
  geom_point(color="red")+
  geom_smooth(method = lm)+
  theme_bw()

plot2
```

# Task4 : Pairwise relationships

```{r,warning=FALSE}
hfi %>% 
  select(ends_with("_score")) %>% 
  ggpairs()
```


4)  - hf_score and pf_score -  strong Linear relationship
    - hf_score and ef_score -  moderate Linear relationship
    - pf_score and ef_score -  no relationship (nuetral)
    
5)  - hf_score and pf_score are highly correlated with correlation-coefficient of 0.943
    - hf_score and ef_score are highly correlated with correlation-coefficient of 0.855
    - ef_score and pf_score are moderately correlated with correlation-coefficient of 0.633
    
## Task5 : The Multiple Linear Regression Model

```{r}
m_hr_ef <- lm(pf_score ~ hf_score + ef_score, data = hfi)
tidy(m_hr_ef)
```

6) The estimated equation for this model is


$$
pf\_score=(1.464213e-11)+ (2.000000e+00)\times hf\_score + (-1.000000e+00)\times ef\_score + \varepsilon
$$


7) When given the values of hf_score and ef_score of a country we can substitute those values in the above equation with y-intercept and slopes of explanatory variables to predict the nearest pf_score approximately.



## Challenege: 3-D Plots

I have used this [Reference](http://www.sthda.com/english/wiki/scatterplot3d-3d-graphics-r-software-and-data-visualization) for 3-D scatterplot visualization

```{r,warning=FALSE}
# Selected only the required columns from the main dataset "hfi" and assigned them to a new dataset "hfi2"

hfi2<-select(hfi,c("pf_score","ef_score","hf_score"))

plot3<-scatterplot3d::scatterplot3d(hfi2[1:100,],main = "3-D Scatterplot",pch = 20,color = "red",grid = FALSE,type ="h",box = FALSE)


# adding a regression plane to the 3-d scatter plot

my.lm <- lm(hfi2$pf_score ~ hfi2$hf_score+ hfi2$ef_score )

plot3$plane3d(my.lm,lty="dashed" ,draw_polygon = TRUE,draw_lines = FALSE)



```


8) yes, both the plots (3D-Scatter plot & ggpairs) displayed in github when I pushed my work there. When comparing those visualizations I found it easy to find the relationships and correlations of the variables in ggpairs plot than the 3D-scatter plot and also the ggpairs plot provides the correlation coefficient of the variables which makes it more easy to take a decision on the relationship of variables. Whereas, in 3D-scatter plot it is easy to visualize all three variables in the same plot.


# Day 2

## Task 2: Overall model - is at least one predictor useful?

```{r}
# review any visual patterns
hfi %>% 
  select(pf_score, pf_expression_influence, pf_expression_control) %>% 
  ggpairs()

#fit the mlr model
m_pf <- lm(pf_score ~ pf_expression_influence + pf_expression_control, data = hfi)
tidy(m_pf)

```


1) The correlation coefficients values (0.787 & 0.796) means that , the pf_expression_influence is having a 78.7 percent of positive or direct relationship with pf_score whereas the pf_expression_control is having a 79.6 percent of positive or direct relationship with pf_score.


```{r}
summary(m_pf)
```

2) We can take a decision to reject the null hypothesis from out Hypothesis testing

3) The decision means that at least one of the explanatory variables(pf_expression_influence & pf_expression_control)will be helpful to predict and have a positive relationship with Response variable (pf_score).


## Task 3: Deciding on important variables


4)  The statistic values for pf_expression_influence and pf_expression_control are correct and verified

5) These are T-statistic values

6)  The F-statistic has 2 degrees of freedom for the numerator and 1375 for the denominator
    The T-statistic has 1376 degrees of freedom
    
7) I think it is better to use the F-statistic as per my knowledge


## Task 4: Model fit

```{r}
glance(m_pf)
```

8) The value of R-squared value is 0.6554

9) The R-squared value tells that this model can explain 65.54 percent of variation in the independent variable

10) 


```{r}
# obtain fitted values and residuals
m_pf_aug <- augment(m_pf)

# plot fitted values and residuals
ggplot(data = m_pf_aug, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  xlab("Fitted values") +
  ylab("Residuals")
```

11) There seems to be no particular pattern in the residual plot above, this tells us that the relationship between dependent and independent variables is linear 


```{r}
ggplot(data = m_pf_aug, aes(x = .resid)) +
  geom_histogram(binwidth = 0.25) +
  xlab("Residuals")
```

12) Based on the histogram, the nearly normal residuals condition does not appear to be violated because there is only one peak and the residuals were normally distributed .

13) Based on the residuals vs. fitted plot, the constant variability condition appear is not violated, because there is an roughly equal spread of residuals at each level of fitted values



